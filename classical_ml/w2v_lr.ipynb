{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1: Word2Vec + Logistic Regression\n",
    "\n",
    "- Code version: 1.0\n",
    "- Python version: 3.11.6\n",
    "- Owner: Aditya Patkar\n",
    "- File created: 2023-11-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the JAVA_HOME environment variable to the path of Java installation.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import wandb\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import Tokenizer, NGram, Word2Vec, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mapatkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/aditya/Desktop/MSML/MSML-651/project/wandb/run-20231201_150122-d1n90fhl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/d1n90fhl' target=\"_blank\">w2v_lr</a></strong> to <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis' target=\"_blank\">https://wandb.ai/apatkar/msml651-sentiment-analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/d1n90fhl' target=\"_blank\">https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/d1n90fhl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/d1n90fhl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x130626d50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#login to wandb and initialize the project\n",
    "#wandb.login(relogin=True ) #uncomment this line if you are running this code for the first time\n",
    "wandb.init(project=\"msml651-sentiment-analysis\", entity=\"apatkar\", name=\"w2v_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/01 15:01:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/01 15:01:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/01 15:01:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/12/01 15:01:28 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "#initialize spark context\n",
    "try:\n",
    "    # create SparkContext on all CPUs available)\n",
    "    sc = ps.SparkContext( 'local[*]' )\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data from s3\n",
    "s3 = boto3.resource('s3', region_name='us-east-1', aws_access_key_id=\"AKIAVMCC766MHUJBYMEJ\", aws_secret_access_key=\"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\")\n",
    "bucket = s3.Bucket('msml651')\n",
    "bucket.download_file('sentiment140_clean_no_stopwords.parquet', './data/sentiment140_clean_no_stopwords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+\n",
      "|target|  tweet_id|                date|query_flag|      user_name|               tweet|post_clean_length|pre_clean_length|pre_clean_words|post_clean_words|tweet_without_stopwords|\n",
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+\n",
      "|     0|1467810369|Mon Apr 06 22:19:...|  NO_QUERY|_TheSpecialOne_|awww that s a bum...|               44|             115|             19|               8|   awww bummer shoul...|\n",
      "|     0|1467810672|Mon Apr 06 22:19:...|  NO_QUERY|  scotthamilton|is upset that he ...|               69|             111|             21|              11|   upset update face...|\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|  NO_QUERY|       mattycus|i dived many time...|               49|              89|             18|               9|   dived many times ...|\n",
      "|     0|1467811184|Mon Apr 06 22:19:...|  NO_QUERY|        ElleCTF|my whole body fee...|               32|              47|             10|               6|   whole body feels ...|\n",
      "|     0|1467811193|Mon Apr 06 22:19:...|  NO_QUERY|         Karoli|no it s not behav...|               16|             111|             21|               3|       behaving mad see|\n",
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the data into a spark dataframe\n",
    "df = sqlContext.read.parquet('./data/sentiment140_clean_no_stopwords.parquet')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the config parameters\n",
    "config = {\n",
    "    'type': 'w2v + lr',\n",
    "    'vector_size': 65,\n",
    "    'min_count': 5,\n",
    "    'window_size': 5,\n",
    "    'train_size': 0.95,\n",
    "    'test_size': 0.25,\n",
    "    'val_size': 0.25,\n",
    "    'max_iter': 100,\n",
    "    'reg_param': 0.1,\n",
    "    'elastic_net_param': 0.1,\n",
    "    \n",
    "}\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train, test and validation sets\n",
    "(train_set, val_set, test_set) = df.randomSplit([config['train_size'], config['val_size'], config['test_size']], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_w2v_pipeline(input_column = 'tweet_without_stopwords', target_column = 'target', n=2):\n",
    "    \"\"\"\n",
    "    Create Word2Vec pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    #tokenize the tweets\n",
    "    tokenizer = Tokenizer(inputCol=input_column, outputCol=\"tokens\")\n",
    "\n",
    "    #create ngrams\n",
    "    ngram = NGram(n=n, inputCol=\"tokens\", outputCol=\"ngrams\")\n",
    "\n",
    "    #create word2vec\n",
    "    word2Vec = Word2Vec(vectorSize=config['vector_size'], minCount=config['min_count'], inputCol=\"ngrams\", outputCol=\"features\")\n",
    "    \n",
    "    #label\n",
    "    label_stringIdx = StringIndexer(inputCol = target_column, outputCol = 'label')\n",
    "    \n",
    "    #lr\n",
    "    lr = LogisticRegression(maxIter=config['max_iter'], regParam=config['reg_param'], elasticNetParam=config['elastic_net_param'], featuresCol='features', labelCol='label')\n",
    "    \n",
    "    #create the pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, ngram, word2Vec, label_stringIdx, lr])\n",
    "    \n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = create_w2v_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#fit the pipeline to the training data and transform the data\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+----------+--------------------+\n",
      "|tweet_without_stopwords|       rawPrediction|prediction|         probability|\n",
      "+-----------------------+--------------------+----------+--------------------+\n",
      "|   upset update face...|[0.22778971622185...|       0.0|[0.55670245866520...|\n",
      "|   hey long time see...|[-0.1348445418892...|       1.0|[0.46633985275736...|\n",
      "|                   nope|[-0.1017402144226...|       1.0|[0.47458686371214...|\n",
      "|      day get much done|[1.06885400881405...|       0.0|[0.74437891891001...|\n",
      "|      im sad miss lilly|[1.21706153282713...|       0.0|[0.77154602042009...|\n",
      "|   hacked account ai...|[-0.1252114518831...|       1.0|[0.46873796996256...|\n",
      "|   want go promote g...|[0.27330963887894...|       0.0|[0.56790523536298...|\n",
      "|   wow tons replies ...|[-0.0185768140466...|       1.0|[0.49535593004251...|\n",
      "|   leaving parking l...|[0.10916100303523...|       0.0|[0.52726318350969...|\n",
      "|   sure right need s...|[0.41175833383571...|       0.0|[0.60150941928131...|\n",
      "+-----------------------+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select('tweet_without_stopwords', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on validation data = 0.65797\n"
     ]
    }
   ],
   "source": [
    "#evaluate the predictions\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\") #create an evaluator\n",
    "auc = evaluator.evaluate(predictions) #evaluate the predictions, this is the AUC\n",
    "print(\"AUC on validation data = %g\" % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 62832.  74862.]\n",
      " [ 33786. 104416.]]\n",
      "Accuracy: 0.6061994374691912\n",
      "Precision for negative: 0.5824250605205323\n",
      "Recall for negative: 0.7555317578616807\n",
      "F1-Score for negative: 0.6577800176389064\n",
      "Precision for positive: 0.6503136061603428\n",
      "Recall for positive: 0.45631617935422025\n",
      "F1-Score for positive: 0.5363105602786029\n",
      "Macro Precision: 0.6163693333404376\n",
      "Macro Recall: 0.6059239686079505\n",
      "Macro F1-Score: 0.5970452889587546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd #get the predictions and labels as an rdd because the MulticlassMetrics class needs an rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "# Get confusion matrix\n",
    "print(metrics.confusionMatrix().toArray()) \n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy: %s\" % (metrics.accuracy))\n",
    "\n",
    "# Get precision, recall, f1\n",
    "\n",
    "print(\"Precision for negative: %s\" % (metrics.precision(label=1.0)))\n",
    "print(\"Recall for negative: %s\" % (metrics.recall(label=1.0)))\n",
    "print(\"F1-Score for negative: %s\" % (metrics.fMeasure(label=1.0, beta=1.0)))\n",
    "\n",
    "print(\"Precision for positive: %s\" % (metrics.precision(label=0.0)))\n",
    "print(\"Recall for positive: %s\" % (metrics.recall(label=0.0)))\n",
    "print(\"F1-Score for positive: %s\" % (metrics.fMeasure(label=0.0, beta=1.0)))\n",
    "\n",
    "# calculate macro avg\n",
    "precision = (metrics.precision(label=1.0) + metrics.precision(label=0.0))/2\n",
    "recall = (metrics.recall(label=1.0) + metrics.recall(label=0.0))/2\n",
    "f1 = (metrics.fMeasure(label=1.0, beta=1.0) + metrics.fMeasure(label=0.0, beta=1.0))/2\n",
    "\n",
    "print(\"Macro Precision: %s\" % (precision))\n",
    "print(\"Macro Recall: %s\" % (recall))\n",
    "print(\"Macro F1-Score: %s\" % (f1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3f581a359c490796757ed9446c119f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>f1_negative</td><td>▁</td></tr><tr><td>f1_positive</td><td>▁</td></tr><tr><td>macro_f1</td><td>▁</td></tr><tr><td>macro_precision</td><td>▁</td></tr><tr><td>macro_recall</td><td>▁</td></tr><tr><td>precision_negative</td><td>▁</td></tr><tr><td>precision_positive</td><td>▁</td></tr><tr><td>recall_negative</td><td>▁</td></tr><tr><td>recall_positive</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.6062</td></tr><tr><td>auc</td><td>0.65797</td></tr><tr><td>f1_negative</td><td>0.65778</td></tr><tr><td>f1_positive</td><td>0.53631</td></tr><tr><td>macro_f1</td><td>0.59705</td></tr><tr><td>macro_precision</td><td>0.61637</td></tr><tr><td>macro_recall</td><td>0.60592</td></tr><tr><td>precision_negative</td><td>0.58243</td></tr><tr><td>precision_positive</td><td>0.65031</td></tr><tr><td>recall_negative</td><td>0.75553</td></tr><tr><td>recall_positive</td><td>0.45632</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">w2v_lr</strong> at: <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/d1n90fhl' target=\"_blank\">https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/d1n90fhl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231201_150122-d1n90fhl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log the results\n",
    "wandb.log({\"auc\": auc, \"accuracy\": metrics.accuracy, \"precision_negative\": metrics.precision(label=1.0), \"recall_negative\": metrics.recall(label=1.0), \"f1_negative\": metrics.fMeasure(label=1.0, beta=1.0), \"precision_positive\": metrics.precision(label=0.0), \"recall_positive\": metrics.recall(label=0.0), \"f1_positive\": metrics.fMeasure(label=0.0, beta=1.0), \"macro_precision\": precision, \"macro_recall\": recall, \"macro_f1\": f1})\n",
    "\n",
    "# save the model\n",
    "\n",
    "#pipeline.save(\"svmModel-ngram-tfidf\")\n",
    "\n",
    "# push the model to wandb\n",
    "#wandb.save('svmModel-ngram-tfidf')\n",
    "\n",
    "# finish the run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
