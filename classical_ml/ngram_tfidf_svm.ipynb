{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1: N-Gram + TfIdf using Count Vectorizer + SVM\n",
    "\n",
    "- Code version: 1.0\n",
    "- Python version: 3.11.6\n",
    "- Owner: Aditya Patkar\n",
    "- File created: 2023-11-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the JAVA_HOME environment variable to the path of Java installation.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import wandb\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import IDF, Tokenizer, CountVectorizer, StringIndexer, NGram,  VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login to wandb and initialize the project\n",
    "#wandb.login(relogin=True ) #uncomment this line if you are running this code for the first time\n",
    "wandb.init(project=\"msml651-sentiment-analysis\", entity=\"apatkar\", name=\"tfidf+ngram+svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize spark context\n",
    "try:\n",
    "    # create SparkContext on all CPUs available)\n",
    "    sc = ps.SparkContext( 'local[*]' )\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data from s3\n",
    "s3 = boto3.resource('s3', region_name='us-east-1', aws_access_key_id=\"KEY\", aws_secret_access_key=\"KEY\")\n",
    "bucket = s3.Bucket('msml651')\n",
    "bucket.download_file('sentiment140_clean_no_stopwords.parquet', './data/sentiment140_clean_no_stopwords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data into a spark dataframe\n",
    "df = sqlContext.read.parquet('./data/sentiment140_clean_no_stopwords.parquet')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the config parameters\n",
    "config = {\n",
    "    'train_size': 0.95,\n",
    "    'test_size': 0.025,\n",
    "    'val_size' : 0.025,\n",
    "    'vocab_size': 5000,\n",
    "    'idf_min_doc_freq': 5,\n",
    "    'type': 'tfidf + ngram + svm',\n",
    "    'max_iter': 100,\n",
    "    'reg_param': 0.3,  \n",
    "}\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train, test and validation sets\n",
    "(train_set, val_set, test_set) = df.randomSplit([config['train_size'], config['val_size'], config['test_size']], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(input_column = 'tweet_without_stopwords', target_column = 'target', n=3):\n",
    "    \"\"\"\n",
    "    Create 1 to ngrams from the input column and apply a model to it\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol=input_column, outputCol=\"words\")] \n",
    "    ngrams = [NGram(n=i, inputCol=\"words\", outputCol=f\"{i}_grams\") for i in range(1, n+1)]\n",
    "    cv = [CountVectorizer(vocabSize=config['vocab_size'], inputCol=f\"{i}_grams\", outputCol=f\"{i}_tf\") for i in range(1, n+1)]\n",
    "    idf = [IDF(minDocFreq=config['idf_min_doc_freq'], inputCol=f\"{i}_tf\", outputCol=f\"{i}_tfidf\") for i in range(1, n+1)]\n",
    "    assembler = [VectorAssembler(inputCols=[f\"{i}_tfidf\" for i in range(1, n+1)], outputCol=\"features\")]\n",
    "    label_stringIdx = [StringIndexer(inputCol = target_column, outputCol = 'label')]\n",
    "    svm = [LinearSVC(maxIter=config['max_iter'], regParam=config['reg_param'])]\n",
    "    \n",
    "    pipeline = Pipeline(stages=tokenizer + ngrams + cv + idf + assembler + label_stringIdx + svm)\n",
    "    return pipeline\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = create_ngrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the pipeline to the training data and transform the data\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the predictions\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\") #create an evaluator\n",
    "auc = evaluator.evaluate(predictions) #evaluate the predictions, this is the AUC\n",
    "print(\"AUC on validation data = %g\" % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd #get the predictions and labels as an rdd because the MulticlassMetrics class needs an rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "# Get confusion matrix\n",
    "print(metrics.confusionMatrix().toArray()) \n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy: %s\" % (metrics.accuracy))\n",
    "\n",
    "# Get precision, recall, f1\n",
    "\n",
    "print(\"Precision for negative: %s\" % (metrics.precision(label=1.0)))\n",
    "print(\"Recall for negative: %s\" % (metrics.recall(label=1.0)))\n",
    "print(\"F1-Score for negative: %s\" % (metrics.fMeasure(label=1.0, beta=1.0)))\n",
    "\n",
    "print(\"Precision for positive: %s\" % (metrics.precision(label=0.0)))\n",
    "print(\"Recall for positive: %s\" % (metrics.recall(label=0.0)))\n",
    "print(\"F1-Score for positive: %s\" % (metrics.fMeasure(label=0.0, beta=1.0)))\n",
    "\n",
    "# calculate macro avg\n",
    "precision = (metrics.precision(label=1.0) + metrics.precision(label=0.0))/2\n",
    "recall = (metrics.recall(label=1.0) + metrics.recall(label=0.0))/2\n",
    "f1 = (metrics.fMeasure(label=1.0, beta=1.0) + metrics.fMeasure(label=0.0, beta=1.0))/2\n",
    "\n",
    "print(\"Macro Precision: %s\" % (precision))\n",
    "print(\"Macro Recall: %s\" % (recall))\n",
    "print(\"Macro F1-Score: %s\" % (f1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the results\n",
    "wandb.log({\"auc\": auc, \"accuracy\": metrics.accuracy, \"precision_negative\": metrics.precision(label=1.0), \"recall_negative\": metrics.recall(label=1.0), \"f1_negative\": metrics.fMeasure(label=1.0, beta=1.0), \"precision_positive\": metrics.precision(label=0.0), \"recall_positive\": metrics.recall(label=0.0), \"f1_positive\": metrics.fMeasure(label=0.0, beta=1.0), \"macro_precision\": precision, \"macro_recall\": recall, \"macro_f1\": f1})\n",
    "\n",
    "# save the model\n",
    "\n",
    "#pipeline.save(\"svmModel-ngram-tfidf\")\n",
    "\n",
    "# push the model to wandb\n",
    "#wandb.save('svmModel-ngram-tfidf')\n",
    "\n",
    "# finish the run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
