{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1: TF-IDF + Logistic Regression\n",
    "\n",
    "- Code version: 1.0\n",
    "- Python version: 3.11.6\n",
    "- Owner: Aditya Patkar\n",
    "- File created: 2023-11-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the JAVA_HOME environment variable to the path of Java installation.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'train_size':0.8, 'test_size':0.2, 'embedding_dim':200, 'dropout':0.2,'batch_size':1024, 'epochs':15, 'factor_lr': 0.1, 'min_lr':0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import wandb\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator \n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/aditya/Desktop/MSML/MSML-651/project/wandb/run-20231201_130355-y8ez2rc8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/y8ez2rc8' target=\"_blank\">tfidf+lr</a></strong> to <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis' target=\"_blank\">https://wandb.ai/apatkar/msml651-sentiment-analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/y8ez2rc8' target=\"_blank\">https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/y8ez2rc8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/y8ez2rc8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1358366d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#login to wandb and initialize the project\n",
    "wandb.login(relogin=True ) #uncomment this line if you are running this code for the first time\n",
    "wandb.init(project=\"msml651-sentiment-analysis\", entity=\"apatkar\", name=\"tfidf+lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/60/v6cyclss2sv55mk0676c81s80000gn/T/ipykernel_99557/3719209557.py:8: UserWarning: SparkContext already exists in this scope\n",
      "  warnings.warn(\"SparkContext already exists in this scope\")\n"
     ]
    }
   ],
   "source": [
    "#initialize spark context\n",
    "try:\n",
    "    # create SparkContext on all CPUs available)\n",
    "    sc = ps.SparkContext( 'local[*]' )\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data from s3\n",
    "s3 = boto3.resource('s3', region_name='us-east-1', aws_access_key_id=\"AKIAVMCC766MHUJBYMEJ\", aws_secret_access_key=\"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\")\n",
    "bucket = s3.Bucket('msml651')\n",
    "bucket.download_file('sentiment140_clean_no_stopwords.parquet', './data/sentiment140_clean_no_stopwords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+\n",
      "|target|  tweet_id|                date|query_flag|      user_name|               tweet|post_clean_length|pre_clean_length|pre_clean_words|post_clean_words|tweet_without_stopwords|\n",
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+\n",
      "|     0|1467810369|Mon Apr 06 22:19:...|  NO_QUERY|_TheSpecialOne_|awww that s a bum...|               44|             115|             19|               8|   awww bummer shoul...|\n",
      "|     0|1467810672|Mon Apr 06 22:19:...|  NO_QUERY|  scotthamilton|is upset that he ...|               69|             111|             21|              11|   upset update face...|\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|  NO_QUERY|       mattycus|i dived many time...|               49|              89|             18|               9|   dived many times ...|\n",
      "|     0|1467811184|Mon Apr 06 22:19:...|  NO_QUERY|        ElleCTF|my whole body fee...|               32|              47|             10|               6|   whole body feels ...|\n",
      "|     0|1467811193|Mon Apr 06 22:19:...|  NO_QUERY|         Karoli|no it s not behav...|               16|             111|             21|               3|       behaving mad see|\n",
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the data into a spark dataframe\n",
    "df = sqlContext.read.parquet('./data/sentiment140_clean_no_stopwords.parquet')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the config parameters\n",
    "config = {\n",
    "    'reg_param': 0.001, \n",
    "    'max_iter': 200,   \n",
    "    'elastic_net_param': 0.001,\n",
    "    'train_size': 0.95,\n",
    "    'test_size': 0.025,\n",
    "    'val_size' : 0.025,\n",
    "    'tf_hash_size': 2**16,\n",
    "    'idf_min_doc_freq': 5,\n",
    "    'type': 'tfidf + lr',\n",
    "}\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train, test and validation sets\n",
    "(train_set, val_set, test_set) = df.randomSplit([config['train_size'], config['val_size'], config['test_size']], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline to transform the data \n",
    "tokenizer = Tokenizer(inputCol=\"tweet_without_stopwords\", outputCol=\"words\") #split tweet into words\n",
    "hashtf = HashingTF(numFeatures=config['tf_hash_size'], inputCol=\"words\", outputCol='tf') #hash the words into vectors, num features is the number of buckets to hash into\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=config['idf_min_doc_freq']) #use idf to reduce the importance of words that appear frequently\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\") #convert the target into a label\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx]) #create a pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 13:04:55 WARN DAGScheduler: Broadcasting large task binary with size 1105.5 KiB\n",
      "23/12/01 13:04:55 WARN DAGScheduler: Broadcasting large task binary with size 1105.5 KiB\n",
      "[Stage 121:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+--------------------+--------------------+--------------------+-----+\n",
      "|target|  tweet_id|                date|query_flag|      user_name|               tweet|post_clean_length|pre_clean_length|pre_clean_words|post_clean_words|tweet_without_stopwords|               words|                  tf|            features|label|\n",
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+--------------------+--------------------+--------------------+-----+\n",
      "|     0|1467810369|Mon Apr 06 22:19:...|  NO_QUERY|_TheSpecialOne_|awww that s a bum...|               44|             115|             19|               8|   awww bummer shoul...|[awww, bummer, sh...|(65536,[21640,272...|(65536,[21640,272...|  0.0|\n",
      "|     0|1467810672|Mon Apr 06 22:19:...|  NO_QUERY|  scotthamilton|is upset that he ...|               69|             111|             21|              11|   upset update face...|[upset, update, f...|(65536,[16064,193...|(65536,[16064,193...|  0.0|\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|  NO_QUERY|       mattycus|i dived many time...|               49|              89|             18|               9|   dived many times ...|[dived, many, tim...|(65536,[2548,2888...|(65536,[2548,2888...|  0.0|\n",
      "|     0|1467811184|Mon Apr 06 22:19:...|  NO_QUERY|        ElleCTF|my whole body fee...|               32|              47|             10|               6|   whole body feels ...|[whole, body, fee...|(65536,[11650,151...|(65536,[11650,151...|  0.0|\n",
      "|     0|1467811193|Mon Apr 06 22:19:...|  NO_QUERY|         Karoli|no it s not behav...|               16|             111|             21|               3|       behaving mad see|[behaving, mad, see]|(65536,[1968,8538...|(65536,[1968,8538...|  0.0|\n",
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#fit the pipeline to the training data and transform the data\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 12:54:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label|target|\n",
      "+-----+------+\n",
      "|  0.0|     4|\n",
      "|  1.0|     0|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#unique labels and the target for context\n",
    "train_df.select(\"label\", \"target\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 13:05:03 WARN DAGScheduler: Broadcasting large task binary with size 1121.8 KiB\n",
      "23/12/01 13:05:13 WARN DAGScheduler: Broadcasting large task binary with size 1123.0 KiB\n",
      "23/12/01 13:05:13 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:24 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:24 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:25 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:30 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:31 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:31 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:31 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:31 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:31 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n",
      "23/12/01 13:05:31 WARN DAGScheduler: Broadcasting large task binary with size 1123.6 KiB\n",
      "23/12/01 13:05:31 WARN DAGScheduler: Broadcasting large task binary with size 1625.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on validation data = 0.841009\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=config['max_iter'], regParam=config['reg_param'], elasticNetParam=config['elastic_net_param']) #create a logistic regression model\n",
    "lrModel = lr.fit(train_df) #fit the model to the training data\n",
    "predictions = lrModel.transform(val_df) #use the model to make predictions on the validation data\n",
    "\n",
    "#evaluate the predictions\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\") #create an evaluator\n",
    "auc = evaluator.evaluate(predictions) #evaluate the predictions, this is the AUC\n",
    "print(\"AUC on validation data = %g\" % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the ROC curve\n",
    "results = pipelineFit.stages[-1].summary.roc.select('FPR', 'TPR').toPandas()\n",
    "plt.plot(results['FPR'], results['TPR'])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# save to wandb\n",
    "wandb.log({\"roc\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/miniforge3/envs/msml651/lib/python3.11/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "23/12/01 13:06:08 WARN DAGScheduler: Broadcasting large task binary with size 1625.2 KiB\n",
      "23/12/01 13:06:08 WARN DAGScheduler: Broadcasting large task binary with size 1625.2 KiB\n",
      "23/12/01 13:06:11 WARN DAGScheduler: Broadcasting large task binary with size 1638.5 KiB\n",
      "[Stage 219:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15023.  5061.]\n",
      " [ 4295. 15821.]]\n",
      "Accuracy: 0.7672636815920398\n",
      "Precision for negative: 0.7576381572646298\n",
      "Recall for negative: 0.7864883674686817\n",
      "F1-Score for negative: 0.7717937460363921\n",
      "Precision for positive: 0.777668495703489\n",
      "Recall for positive: 0.7480083648675563\n",
      "F1-Score for positive: 0.7625501243591695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd #get the predictions and labels as an rdd because the MulticlassMetrics class needs an rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "# Get confusion matrix\n",
    "print(metrics.confusionMatrix().toArray()) \n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy: %s\" % (metrics.accuracy))\n",
    "\n",
    "# Get precision, recall, f1\n",
    "\n",
    "print(\"Precision for negative: %s\" % (metrics.precision(label=1.0)))\n",
    "print(\"Recall for negative: %s\" % (metrics.recall(label=1.0)))\n",
    "print(\"F1-Score for negative: %s\" % (metrics.fMeasure(label=1.0, beta=1.0)))\n",
    "\n",
    "print(\"Precision for positive: %s\" % (metrics.precision(label=0.0)))\n",
    "print(\"Recall for positive: %s\" % (metrics.recall(label=0.0)))\n",
    "print(\"F1-Score for positive: %s\" % (metrics.fMeasure(label=0.0, beta=1.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a7b60d65234f799486f5dd2f2905b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>f1_negative</td><td>▁</td></tr><tr><td>f1_positive</td><td>▁</td></tr><tr><td>precision_negative</td><td>▁</td></tr><tr><td>precision_positive</td><td>▁</td></tr><tr><td>recall_negative</td><td>▁</td></tr><tr><td>recall_positive</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.76726</td></tr><tr><td>auc</td><td>0.84101</td></tr><tr><td>f1_negative</td><td>0.77179</td></tr><tr><td>f1_positive</td><td>0.76255</td></tr><tr><td>precision_negative</td><td>0.75764</td></tr><tr><td>precision_positive</td><td>0.77767</td></tr><tr><td>recall_negative</td><td>0.78649</td></tr><tr><td>recall_positive</td><td>0.74801</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tfidf+lr</strong> at: <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/y8ez2rc8' target=\"_blank\">https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/y8ez2rc8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231201_130355-y8ez2rc8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log the results\n",
    "wandb.log({\"auc\": auc, \"accuracy\": metrics.accuracy, \"precision_negative\": metrics.precision(label=1.0), \"recall_negative\": metrics.recall(label=1.0), \"f1_negative\": metrics.fMeasure(label=1.0, beta=1.0), \"precision_positive\": metrics.precision(label=0.0), \"recall_positive\": metrics.recall(label=0.0), \"f1_positive\": metrics.fMeasure(label=0.0, beta=1.0)})\n",
    "\n",
    "# save the model\n",
    "\n",
    "#lrModel.save(\"lrModel\")\n",
    "\n",
    "# push the model to wandb\n",
    "#wandb.save('lrModel')\n",
    "\n",
    "# finish the run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
