{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1: TF-IDF + Logistic Regression\n",
    "\n",
    "- Code version: 1.0\n",
    "- Python version: 3.11.6\n",
    "- Owner: Aditya Patkar\n",
    "- File created: 2023-11-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the JAVA_HOME environment variable to the path of Java installation.\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import wandb\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator \n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login to wandb and initialize the project\n",
    "wandb.login(relogin=True ) #uncomment this line if you are running this code for the first time\n",
    "wandb.init(project=\"msml651-sentiment-analysis\", entity=\"apatkar\", name=\"tfidf+lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize spark context\n",
    "try:\n",
    "    # create SparkContext on all CPUs available)\n",
    "    sc = ps.SparkContext( 'local[*]' )\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data from s3\n",
    "s3 = boto3.resource('s3', region_name='us-east-1', aws_access_key_id=\"KEY\", aws_secret_access_key=\"KEY\")\n",
    "bucket = s3.Bucket('msml651')\n",
    "bucket.download_file('sentiment140_clean_no_stopwords.parquet', './data/sentiment140_clean_no_stopwords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data into a spark dataframe\n",
    "df = sqlContext.read.parquet('./data/sentiment140_clean_no_stopwords.parquet')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the config parameters\n",
    "config = {\n",
    "    'reg_param': 0.001, \n",
    "    'max_iter': 200,   \n",
    "    'elastic_net_param': 0.001,\n",
    "    'train_size': 0.95,\n",
    "    'test_size': 0.025,\n",
    "    'val_size' : 0.025,\n",
    "    'tf_hash_size': 2**16,\n",
    "    'idf_min_doc_freq': 5,\n",
    "    'type': 'tfidf + lr',\n",
    "}\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train, test and validation sets\n",
    "(train_set, val_set, test_set) = df.randomSplit([config['train_size'], config['val_size'], config['test_size']], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline to transform the data \n",
    "tokenizer = Tokenizer(inputCol=\"tweet_without_stopwords\", outputCol=\"words\") #split tweet into words\n",
    "hashtf = HashingTF(numFeatures=config['tf_hash_size'], inputCol=\"words\", outputCol='tf') #hash the words into vectors, num features is the number of buckets to hash into\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=config['idf_min_doc_freq']) #use idf to reduce the importance of words that appear frequently\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\") #convert the target into a label\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx]) #create a pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the pipeline to the training data and transform the data\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique labels and the target for context\n",
    "train_df.select(\"label\", \"target\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=config['max_iter'], regParam=config['reg_param'], elasticNetParam=config['elastic_net_param']) #create a logistic regression model\n",
    "lrModel = lr.fit(train_df) #fit the model to the training data\n",
    "predictions = lrModel.transform(val_df) #use the model to make predictions on the validation data\n",
    "\n",
    "#evaluate the predictions\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\") #create an evaluator\n",
    "auc = evaluator.evaluate(predictions) #evaluate the predictions, this is the AUC\n",
    "print(\"AUC on validation data = %g\" % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the ROC curve\n",
    "results = pipelineFit.stages[-1].summary.roc.select('FPR', 'TPR').toPandas()\n",
    "plt.plot(results['FPR'], results['TPR'])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# save to wandb\n",
    "wandb.log({\"roc\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd #get the predictions and labels as an rdd because the MulticlassMetrics class needs an rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "# Get confusion matrix\n",
    "print(metrics.confusionMatrix().toArray()) \n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy: %s\" % (metrics.accuracy))\n",
    "\n",
    "# Get precision, recall, f1\n",
    "\n",
    "print(\"Precision for negative: %s\" % (metrics.precision(label=1.0)))\n",
    "print(\"Recall for negative: %s\" % (metrics.recall(label=1.0)))\n",
    "print(\"F1-Score for negative: %s\" % (metrics.fMeasure(label=1.0, beta=1.0)))\n",
    "\n",
    "print(\"Precision for positive: %s\" % (metrics.precision(label=0.0)))\n",
    "print(\"Recall for positive: %s\" % (metrics.recall(label=0.0)))\n",
    "print(\"F1-Score for positive: %s\" % (metrics.fMeasure(label=0.0, beta=1.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the results\n",
    "wandb.log({\"auc\": auc, \"accuracy\": metrics.accuracy, \"precision_negative\": metrics.precision(label=1.0), \"recall_negative\": metrics.recall(label=1.0), \"f1_negative\": metrics.fMeasure(label=1.0, beta=1.0), \"precision_positive\": metrics.precision(label=0.0), \"recall_positive\": metrics.recall(label=0.0), \"f1_positive\": metrics.fMeasure(label=0.0, beta=1.0)})\n",
    "\n",
    "# save the model\n",
    "\n",
    "#lrModel.save(\"lrModel\")\n",
    "\n",
    "# push the model to wandb\n",
    "#wandb.save('lrModel')\n",
    "\n",
    "# finish the run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
