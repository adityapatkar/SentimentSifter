{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1: TfIdf + Random Forest + Ngram\n",
    "\n",
    "- Code version: 1.0\n",
    "- Python version: 3.11.6\n",
    "- Owner: Aditya Patkar\n",
    "- File created: 2023-11-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the JAVA_HOME environment variable to the path of Java installation.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import wandb\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import boto3\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import IDF, Tokenizer, CountVectorizer, StringIndexer, NGram,  VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mapatkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/aditya/Desktop/MSML/MSML-651/project/wandb/run-20231204_114105-ix4wbt3p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/ix4wbt3p' target=\"_blank\">rf+tfidf+ngram</a></strong> to <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis' target=\"_blank\">https://wandb.ai/apatkar/msml651-sentiment-analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/ix4wbt3p' target=\"_blank\">https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/ix4wbt3p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/ix4wbt3p?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x12add7d50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#login to wandb and initialize the project\n",
    "#wandb.login(relogin=True ) #uncomment this line if you are running this code for the first time\n",
    "wandb.init(project=\"msml651-sentiment-analysis\", entity=\"apatkar\", name=\"rf+tfidf+ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/04 11:41:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "#initialize spark context\n",
    "try:\n",
    "    # create SparkContext on all CPUs available)\n",
    "    sc = ps.SparkContext( 'local[*]' )\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/aditya/Desktop/MSML/MSML-651/project/ngram_tfidf_rf.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aditya/Desktop/MSML/MSML-651/project/ngram_tfidf_rf.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m s3 \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mresource(\u001b[39m'\u001b[39m\u001b[39ms3\u001b[39m\u001b[39m'\u001b[39m, region_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mus-east-1\u001b[39m\u001b[39m'\u001b[39m, aws_access_key_id\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAKIAVMCC766MHUJBYMEJ\u001b[39m\u001b[39m\"\u001b[39m, aws_secret_access_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mat7WntH0OBdOy1S4bsrvxyzTJVF5K/TanaRIPEyv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aditya/Desktop/MSML/MSML-651/project/ngram_tfidf_rf.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bucket \u001b[39m=\u001b[39m s3\u001b[39m.\u001b[39mBucket(\u001b[39m'\u001b[39m\u001b[39mmsml651\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aditya/Desktop/MSML/MSML-651/project/ngram_tfidf_rf.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m bucket\u001b[39m.\u001b[39;49mdownload_file(\u001b[39m'\u001b[39;49m\u001b[39msentiment140_clean_no_stopwords.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m./data/sentiment140_clean_no_stopwords.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/msml651/lib/python3.11/site-packages/boto3/s3/inject.py:279\u001b[0m, in \u001b[0;36mbucket_download_file\u001b[0;34m(self, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbucket_download_file\u001b[39m(\n\u001b[1;32m    246\u001b[0m     \u001b[39mself\u001b[39m, Key, Filename, ExtraArgs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, Callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, Config\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    247\u001b[0m ):\n\u001b[1;32m    248\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[39m    Usage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39m        transfer.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeta\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mdownload_file(\n\u001b[1;32m    280\u001b[0m         Bucket\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    281\u001b[0m         Key\u001b[39m=\u001b[39;49mKey,\n\u001b[1;32m    282\u001b[0m         Filename\u001b[39m=\u001b[39;49mFilename,\n\u001b[1;32m    283\u001b[0m         ExtraArgs\u001b[39m=\u001b[39;49mExtraArgs,\n\u001b[1;32m    284\u001b[0m         Callback\u001b[39m=\u001b[39;49mCallback,\n\u001b[1;32m    285\u001b[0m         Config\u001b[39m=\u001b[39;49mConfig,\n\u001b[1;32m    286\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/msml651/lib/python3.11/site-packages/boto3/s3/inject.py:192\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[39mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m    transfer.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mwith\u001b[39;00m S3Transfer(\u001b[39mself\u001b[39m, Config) \u001b[39mas\u001b[39;00m transfer:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m transfer\u001b[39m.\u001b[39;49mdownload_file(\n\u001b[1;32m    193\u001b[0m         bucket\u001b[39m=\u001b[39;49mBucket,\n\u001b[1;32m    194\u001b[0m         key\u001b[39m=\u001b[39;49mKey,\n\u001b[1;32m    195\u001b[0m         filename\u001b[39m=\u001b[39;49mFilename,\n\u001b[1;32m    196\u001b[0m         extra_args\u001b[39m=\u001b[39;49mExtraArgs,\n\u001b[1;32m    197\u001b[0m         callback\u001b[39m=\u001b[39;49mCallback,\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/msml651/lib/python3.11/site-packages/boto3/s3/transfer.py:405\u001b[0m, in \u001b[0;36mS3Transfer.download_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    401\u001b[0m future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_manager\u001b[39m.\u001b[39mdownload(\n\u001b[1;32m    402\u001b[0m     bucket, key, filename, extra_args, subscribers\n\u001b[1;32m    403\u001b[0m )\n\u001b[1;32m    404\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    406\u001b[0m \u001b[39m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[39m# exceeded we need to throw the same error from boto3 instead of\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39m# s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[39m# catching the boto3 one instead of the s3transfer exception to do\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m# their own retries.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mexcept\u001b[39;00m S3TransferRetriesExceededError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/msml651/lib/python3.11/site-packages/s3transfer/futures.py:106\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel()\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniforge3/envs/msml651/lib/python3.11/site-packages/s3transfer/futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[39m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[39m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[39m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_coordinator\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    104\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniforge3/envs/msml651/lib/python3.11/site-packages/s3transfer/futures.py:261\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Waits until TransferFuture is done and returns the result\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[39mIf the TransferFuture succeeded, it will return the result. If the\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mTransferFuture failed, it will raise the exception associated to the\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39mfailure.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# Doing a wait() with no timeout cannot be interrupted in python2 but\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# can be interrupted in python3 so we just wait with the largest\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m# possible value integer value, which is on the scale of billions of\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39m# years...\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_done_event\u001b[39m.\u001b[39;49mwait(MAXINT)\n\u001b[1;32m    263\u001b[0m \u001b[39m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# final result.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n",
      "File \u001b[0;32m~/miniforge3/envs/msml651/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    630\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniforge3/envs/msml651/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    328\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#get the data from s3\n",
    "s3 = boto3.resource('s3', region_name='us-east-1', aws_access_key_id=\"AKIAVMCC766MHUJBYMEJ\", aws_secret_access_key=\"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\")\n",
    "bucket = s3.Bucket('msml651')\n",
    "bucket.download_file('sentiment140_clean_no_stopwords.parquet', './data/sentiment140_clean_no_stopwords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+\n",
      "|target|  tweet_id|                date|query_flag|      user_name|               tweet|post_clean_length|pre_clean_length|pre_clean_words|post_clean_words|tweet_without_stopwords|\n",
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+\n",
      "|     0|1467810369|Mon Apr 06 22:19:...|  NO_QUERY|_TheSpecialOne_|awww that s a bum...|               44|             115|             19|               8|   awww bummer shoul...|\n",
      "|     0|1467810672|Mon Apr 06 22:19:...|  NO_QUERY|  scotthamilton|is upset that he ...|               69|             111|             21|              11|   upset update face...|\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|  NO_QUERY|       mattycus|i dived many time...|               49|              89|             18|               9|   dived many times ...|\n",
      "|     0|1467811184|Mon Apr 06 22:19:...|  NO_QUERY|        ElleCTF|my whole body fee...|               32|              47|             10|               6|   whole body feels ...|\n",
      "|     0|1467811193|Mon Apr 06 22:19:...|  NO_QUERY|         Karoli|no it s not behav...|               16|             111|             21|               3|       behaving mad see|\n",
      "+------+----------+--------------------+----------+---------------+--------------------+-----------------+----------------+---------------+----------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the data into a spark dataframe\n",
    "df = sqlContext.read.parquet('./data/sentiment140_clean_no_stopwords.parquet')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the config parameters\n",
    "config = {\n",
    "    'type': 'tfidf +ngram + rf',\n",
    "    'train_size': 0.95,\n",
    "    'test_size': 0.25,\n",
    "    'val_size': 0.25,\n",
    "    'max_depth': 5,\n",
    "    'vocab_size': 10000,\n",
    "    'idf_min_doc_freq': 5,\n",
    "}\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train, test and validation sets\n",
    "(train_set, val_set, test_set) = df.randomSplit([config['train_size'], config['val_size'], config['test_size']], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rf_pipeline(input_column = 'tweet_without_stopwords', target_column = 'target', n=2):\n",
    "    \"\"\"\n",
    "    Create Random Forest pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = [Tokenizer(inputCol=input_column, outputCol=\"words\")] \n",
    "    ngrams = [NGram(n=i, inputCol=\"words\", outputCol=f\"{i}_grams\") for i in range(1, n+1)]\n",
    "    cv = [CountVectorizer(vocabSize=config['vocab_size'], inputCol=f\"{i}_grams\", outputCol=f\"{i}_tf\") for i in range(1, n+1)]\n",
    "    idf = [IDF(minDocFreq=config['idf_min_doc_freq'], inputCol=f\"{i}_tf\", outputCol=f\"{i}_tfidf\") for i in range(1, n+1)]\n",
    "    assembler = [VectorAssembler(inputCols=[f\"{i}_tfidf\" for i in range(1, n+1)], outputCol=\"features\")]\n",
    "    label_stringIdx = [StringIndexer(inputCol = target_column, outputCol = 'label')]\n",
    "    rf = [RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=config['max_depth'])]\n",
    "    #create the pipeline\n",
    "    pipeline = Pipeline(stages=tokenizer + ngrams + cv + idf + assembler + label_stringIdx + rf)\n",
    "    \n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = create_rf_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#fit the pipeline to the training data and transform the data\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+----------+--------------------+\n",
      "|tweet_without_stopwords|       rawPrediction|prediction|         probability|\n",
      "+-----------------------+--------------------+----------+--------------------+\n",
      "|   upset update face...|[9.99801366078643...|       1.0|[0.49990068303932...|\n",
      "|   hey long time see...|[8.79347892866582...|       1.0|[0.43967394643329...|\n",
      "|                   nope|[9.99801366078643...|       1.0|[0.49990068303932...|\n",
      "|      day get much done|[9.99801366078643...|       1.0|[0.49990068303932...|\n",
      "|      im sad miss lilly|[9.99801366078643...|       1.0|[0.49990068303932...|\n",
      "|   hacked account ai...|[9.71457499730105...|       1.0|[0.48572874986505...|\n",
      "|   want go promote g...|[10.4798655889933...|       0.0|[0.52399327944966...|\n",
      "|   wow tons replies ...|[9.99801366078643...|       1.0|[0.49990068303932...|\n",
      "|   leaving parking l...|[10.2093155779572...|       0.0|[0.51046577889786...|\n",
      "|   sure right need s...|[9.99801366078643...|       1.0|[0.49990068303932...|\n",
      "+-----------------------+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select('tweet_without_stopwords', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on validation data = 0.670186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#evaluate the predictions\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\") #create an evaluator\n",
    "auc = evaluator.evaluate(predictions) #evaluate the predictions, this is the AUC\n",
    "print(\"AUC on validation data = %g\" % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 47636.  90058.]\n",
      " [ 18595. 119607.]]\n",
      "Accuracy: 0.6061813146982921\n",
      "Precision for negative: 0.5704671738249112\n",
      "Recall for negative: 0.8654505723506172\n",
      "F1-Score for negative: 0.6876593640673015\n",
      "Precision for positive: 0.7192402349353022\n",
      "Recall for positive: 0.3459555245689718\n",
      "F1-Score for positive: 0.46719136937599604\n",
      "Macro Precision: 0.6448537043801067\n",
      "Macro Recall: 0.6057030484597945\n",
      "Macro F1-Score: 0.5774253667216488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd #get the predictions and labels as an rdd because the MulticlassMetrics class needs an rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "\n",
    "# Get confusion matrix\n",
    "print(metrics.confusionMatrix().toArray()) \n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy: %s\" % (metrics.accuracy))\n",
    "\n",
    "# Get precision, recall, f1\n",
    "\n",
    "print(\"Precision for negative: %s\" % (metrics.precision(label=1.0)))\n",
    "print(\"Recall for negative: %s\" % (metrics.recall(label=1.0)))\n",
    "print(\"F1-Score for negative: %s\" % (metrics.fMeasure(label=1.0, beta=1.0)))\n",
    "\n",
    "print(\"Precision for positive: %s\" % (metrics.precision(label=0.0)))\n",
    "print(\"Recall for positive: %s\" % (metrics.recall(label=0.0)))\n",
    "print(\"F1-Score for positive: %s\" % (metrics.fMeasure(label=0.0, beta=1.0)))\n",
    "\n",
    "# calculate macro avg\n",
    "precision = (metrics.precision(label=1.0) + metrics.precision(label=0.0))/2\n",
    "recall = (metrics.recall(label=1.0) + metrics.recall(label=0.0))/2\n",
    "f1 = (metrics.fMeasure(label=1.0, beta=1.0) + metrics.fMeasure(label=0.0, beta=1.0))/2\n",
    "\n",
    "print(\"Macro Precision: %s\" % (precision))\n",
    "print(\"Macro Recall: %s\" % (recall))\n",
    "print(\"Macro F1-Score: %s\" % (f1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77243b28a3bc4abba5fa91f3edf42261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>f1_negative</td><td>▁</td></tr><tr><td>f1_positive</td><td>▁</td></tr><tr><td>macro_f1</td><td>▁</td></tr><tr><td>macro_precision</td><td>▁</td></tr><tr><td>macro_recall</td><td>▁</td></tr><tr><td>precision_negative</td><td>▁</td></tr><tr><td>precision_positive</td><td>▁</td></tr><tr><td>recall_negative</td><td>▁</td></tr><tr><td>recall_positive</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.63467</td></tr><tr><td>auc</td><td>0.67156</td></tr><tr><td>f1_negative</td><td>0.69984</td></tr><tr><td>f1_positive</td><td>0.53334</td></tr><tr><td>macro_f1</td><td>0.61659</td></tr><tr><td>macro_precision</td><td>0.66516</td></tr><tr><td>macro_recall</td><td>0.63427</td></tr><tr><td>precision_negative</td><td>0.59466</td></tr><tr><td>precision_positive</td><td>0.73565</td></tr><tr><td>recall_negative</td><td>0.85024</td></tr><tr><td>recall_positive</td><td>0.4183</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">w2v_lr</strong> at: <a href='https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/wyb1gf5r' target=\"_blank\">https://wandb.ai/apatkar/msml651-sentiment-analysis/runs/wyb1gf5r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231201_151017-wyb1gf5r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log the results\n",
    "wandb.log({\"auc\": auc, \"accuracy\": metrics.accuracy, \"precision_negative\": metrics.precision(label=1.0), \"recall_negative\": metrics.recall(label=1.0), \"f1_negative\": metrics.fMeasure(label=1.0, beta=1.0), \"precision_positive\": metrics.precision(label=0.0), \"recall_positive\": metrics.recall(label=0.0), \"f1_positive\": metrics.fMeasure(label=0.0, beta=1.0), \"macro_precision\": precision, \"macro_recall\": recall, \"macro_f1\": f1})\n",
    "\n",
    "# save the model\n",
    "\n",
    "pipeline.save(\"RFModel-ngram-tfidf\")\n",
    "\n",
    "# push the model to wandb\n",
    "wandb.save('RFModel-ngram-tfidf')\n",
    "\n",
    "# finish the run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
