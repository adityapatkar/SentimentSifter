{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MSML651 Project: Predicting Sentiment of Tweets Using the Sentiment140 Dataset\n",
        "\n",
        "- Name: Aditya Patkar\n",
        "- UID: 119390818"
      ],
      "metadata": {
        "id": "c6qF8h_n3Hpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "zDgrrahGKKeE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HpRmqLNgWF7"
      },
      "outputs": [],
      "source": [
        "#install required libraries\n",
        "!pip install awscli boto3 wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and config"
      ],
      "metadata": {
        "id": "co8Y62_83cW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up required secrets\n",
        "from google.colab import userdata\n",
        "access_key = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "secret_key = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "wandb_key = userdata.get('WANDB_KEY')\n",
        "huggingface_key = userdata.get('HUGGINGFACE_KEY')"
      ],
      "metadata": {
        "id": "7rQ7JN_Qr-0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WandB Config\n",
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "wandb.login(relogin=True, key=wandb_key) #uncomment this line if you are running this code for the first time\n",
        "wandb.init(project=\"msml651-sentiment-analysis\", entity=\"apatkar\", name=\"LSTM\", config={\"bs\": 12})"
      ],
      "metadata": {
        "id": "I4sCTs6Os6Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AWS config\n",
        "!aws configure set aws_access_key_id $access_key\n",
        "!aws configure set aws_secret_access_key $secret_key\n",
        "!aws configure set default.region us-east-1"
      ],
      "metadata": {
        "id": "8QE9kAMG3seX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary imports\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "c-ZiovAXgsNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download required files from AWS\n",
        "!aws s3 cp s3://msml651/sentiment140_clean_no_stopwords.parquet .\n",
        "!aws s3 cp s3://msml651/lstm_tokenizer.pickle .\n",
        "!aws s3 cp s3://msml651/glove.twitter.27B.200d.txt .\n"
      ],
      "metadata": {
        "id": "zX04bq3n3hKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model and data configs\n",
        "config = {'train_size':0.8,\n",
        "          'test_size':0.2,\n",
        "          'embedding_dim':200,\n",
        "          'dropout':0.2,\n",
        "          'batch_size':1024,\n",
        "          'epochs':25,\n",
        "          'patience_lr':2,\n",
        "          'factor_lr': 0.1,\n",
        "          'min_lr':0.0001}\n",
        "wandb.config.update(config)"
      ],
      "metadata": {
        "id": "TD45aljJkZQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "jhUtPLaN4kj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read data\n",
        "df = pd.read_parquet(\"/content/sentiment140_clean_no_stopwords.parquet\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5fqXzsjnhjn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_test_split\n",
        "train_data, test_data = train_test_split(df, test_size = config['test_size'], random_state = 20, stratify = df.target)"
      ],
      "metadata": {
        "id": "RLkWY1s9kg2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode target\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_data['target'].to_list())\n",
        "y_train = encoder.transform(train_data['target'].to_list())\n",
        "y_test = encoder.transform(test_data['target'].to_list())\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "n0L-27Ackp29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load tokenizer\n",
        "with open('/content/lstm_tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "BEe5tofOlXgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(s.split()) for s in train_data['tweet_without_stopwords']])\n",
        "\n",
        "# pad sequences in x_train data set to the max length\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data['tweet_without_stopwords']),\n",
        "                        maxlen = max_length)\n",
        "# pad sequences in x_test data set to the max length\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data['tweet_without_stopwords']),\n",
        "                       maxlen = max_length)"
      ],
      "metadata": {
        "id": "7Qv6IzUzlvRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code courtesy: MachineLearningMastery (NOT MY CODE)\n",
        "\n",
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "    # load embedding into memory, skip first line\n",
        "    file = open(filename,'r',encoding=\"utf-8\")\n",
        "    lines = file.readlines()\n",
        "    file.close()\n",
        "    # create a map of words to vectors\n",
        "    embedding = dict()\n",
        "    for line in lines:\n",
        "        parts = line.split()\n",
        "        # key is string word, value is numpy array for vector\n",
        "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
        "    return embedding\n",
        "\n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        vector = embedding.get(word)\n",
        "        if vector is not None:\n",
        "            weight_matrix[i] = vector\n",
        "    return weight_matrix\n",
        "\n",
        "#get embedding weights\n",
        "embedding_dim = config['embedding_dim']\n",
        "vocab = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "raw_embedding = load_embedding('/content/glove.twitter.27B.200d.txt')\n",
        "embedding_matrix = get_weight_matrix(raw_embedding, vocab)"
      ],
      "metadata": {
        "id": "weHJj1LGm_Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "AD_m2X_C5GbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create an embedding layer using the weights\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            embedding_dim,\n",
        "                            weights = [embedding_matrix],\n",
        "                            input_length = max_length,\n",
        "                            trainable = False)"
      ],
      "metadata": {
        "id": "gRYfFUaFng70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build the lstm\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(config['dropout']))\n",
        "model.add(LSTM(200, dropout = 0.2))\n",
        "model.add(Dense(64, activation='leaky_relu'))\n",
        "model.add(Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "Tz1NHeVWnpVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "AbFn73er5IKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = config['batch_size']\n",
        "EPOCHS = config['epochs']\n",
        "\n",
        "#LR Scheduler\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
        "                              factor = config['factor_lr'],\n",
        "                              patience = config['patience_lr'],\n",
        "                              min_lr = config['min_lr'])\n",
        "\n",
        "#train\n",
        "history = model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS,\n",
        "                    validation_split = 0.1, verbose = 1, callbacks = [reduce_lr, WandbMetricsLogger(), WandbModelCheckpoint(\"models\")])"
      ],
      "metadata": {
        "id": "_NCcVt1onstr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "h3HUw5-N5J4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model\n",
        "score = model.evaluate(x_test, y_test, batch_size = BATCH_SIZE)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "YZ4W9oh1n0Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get predictions\n",
        "y_pred = model.predict(x_test, batch_size=1024, verbose=1)\n",
        "y_pred_lst = [[0] if i <0.5 else [1] for i in y_pred]\n",
        "precision = precision_score(y_test, y_pred_lst, average='binary')\n",
        "recall = recall_score(y_test, y_pred_lst, average='binary')\n",
        "f1 = f1_score(y_test, y_pred_lst, average='binary')\n",
        "\n",
        "print(precision, recall, f1)"
      ],
      "metadata": {
        "id": "d3zzeVfIzWpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_thresholds(y_test, y_pred_og, thresholds=[0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65]):\n",
        "  \"\"\"\n",
        "  This function tests different thresholds to find the best F1 score\n",
        "  \"\"\"\n",
        "  best_threshold = thresholds[0]\n",
        "  best_f1 = 0\n",
        "  best_y_pred = [[0] if i < thresholds[0] else [1] for i in y_pred_og]\n",
        "  for threshold in thresholds:\n",
        "    y_pred = [[0] if i < threshold else [1] for i in y_pred_og]\n",
        "    f1 = f1_score(y_test, y_pred, average='binary')\n",
        "    if f1 > best_f1:\n",
        "      best_f1 = f1\n",
        "      best_threshold = threshold\n",
        "      best_y_pred = y_pred\n",
        "  return best_threshold, best_f1, best_y_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3L2FBjaX65L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find best threshold and calculate metrics\n",
        "best_threshold, best_f1, best_y_pred = test_thresholds(y_test, y_pred)\n",
        "\n",
        "print(f\"Best threshold:{best_threshold}\")\n",
        "print(f\"Best f1:{best_f1}\")\n",
        "precision = precision_score(y_test, best_y_pred, average='binary')\n",
        "recall = recall_score(y_test, best_y_pred, average='binary')\n",
        "f1 = f1_score(y_test, best_y_pred, average='binary')\n",
        "accuracy = accuracy_score(y_test, best_y_pred)\n",
        "print(precision, recall,f1, accuracy)"
      ],
      "metadata": {
        "id": "x3rXiWLq7rze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save Model and Log Metrics"
      ],
      "metadata": {
        "id": "VUIGueft5N3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.log({'accuracy': accuracy, 'loss':score[0], 'precision':precision, 'recall':recall, 'f1':f1, 'threshold':best_threshold})\n",
        "model.save('lstm_sentiment.h5')\n",
        "!aws s3 cp /content/lstm_sentiment.h5 s3://msml651"
      ],
      "metadata": {
        "id": "lWRk6301v9sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "vNK7EvyQ5SJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc,label = 'Training acc', color='red')\n",
        "plt.plot(epochs, val_acc, label = 'Validation acc', color='blue')\n",
        "plt.title('LSTM: Training and validation accuracy')\n",
        "plt.legend()\n",
        "image = plt\n",
        "wandb.log({\"LSTM Accuracy\": wandb.Image(image)})"
      ],
      "metadata": {
        "id": "FAGMysNyn-hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize loss\n",
        "plt.plot(epochs, loss,label = 'Training loss', color='red')\n",
        "plt.plot(epochs, val_loss, label = 'Validation loss', color='blue')\n",
        "plt.title('LSTM: Training and validation loss')\n",
        "plt.legend()\n",
        "image = plt\n",
        "wandb.log({\"LSTM Loss\": wandb.Image(image)})"
      ],
      "metadata": {
        "id": "WQOgI_q02Sge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finish run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "fen157Ci3Nrr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}